{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate neutral and selected SFS vectors of all genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../scripts/')\n",
    "#sys.path\n",
    "import pandas as pd\n",
    "import Search_algorithms as sag\n",
    "import gzip\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_file_path='/scratch/research/references/chlamydomonas/5.3_chlamy_w_organelles_mt_minus/annotation/Creinhardtii_v5.3_223_gene.gff3.gz'\n",
    "merged = pd.read_csv(\"../../data/intermediate_data_from_gff/merged_v5_3_1.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>num_detected</th>\n",
       "      <th>num_sampled</th>\n",
       "      <th>proportion</th>\n",
       "      <th>source</th>\n",
       "      <th>annotation_version</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>gene_symbol</th>\n",
       "      <th>pathway_id</th>\n",
       "      <th>transcript_id_v5.3.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cre01.g000017.t1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['Bajhaiya_2016', 'Kwak_2017']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2.t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cre01.g000033.t1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>['Bajhaiya_2016']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g3.t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cre01.g000050.t1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['Gargouri_2015']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000050.t1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cre01.g000100.t1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['Bajhaiya_2016']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000100.t1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cre01.g000150.t1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>['Bajhaiya_2016']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000150</td>\n",
       "      <td>ZRT2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000150.t1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cre01.g000200.t1.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['Bajhaiya_2016']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000200.t1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cre01.g000250.t1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['Bajhaiya_2016']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000250.t1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cre01.g000300.t1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "      <td>['Bajhaiya_2016']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Creinhardtii PWY-5667', 'Creinhardtii PWY-74...</td>\n",
       "      <td>Cre01.g000300.t1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cre01.g000350.t1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>['Hemme_2014', 'Bajhaiya_2016']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000350.t1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cre01.g000400.t1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['Bajhaiya_2016']</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000400.t1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        transcript_id  num_detected  num_sampled  proportion  \\\n",
       "0  Cre01.g000017.t1.1             0            2       0.000   \n",
       "1  Cre01.g000033.t1.1             1            1       0.125   \n",
       "2  Cre01.g000050.t1.1             0            1       0.000   \n",
       "3  Cre01.g000100.t1.1             0            1       0.000   \n",
       "4  Cre01.g000150.t1.2             1            1       0.125   \n",
       "5  Cre01.g000200.t1.1             0            1       0.000   \n",
       "6  Cre01.g000250.t1.2             0            1       0.000   \n",
       "7  Cre01.g000300.t1.1             1            1       0.125   \n",
       "8  Cre01.g000350.t1.1             1            2       0.125   \n",
       "9  Cre01.g000400.t1.2             0            1       0.000   \n",
       "\n",
       "                            source annotation_version        gene_id  \\\n",
       "0   ['Bajhaiya_2016', 'Kwak_2017']               v5.5  Cre01.g000017   \n",
       "1                ['Bajhaiya_2016']               v5.5  Cre01.g000033   \n",
       "2                ['Gargouri_2015']               v5.5  Cre01.g000050   \n",
       "3                ['Bajhaiya_2016']               v5.5  Cre01.g000100   \n",
       "4                ['Bajhaiya_2016']               v5.5  Cre01.g000150   \n",
       "5                ['Bajhaiya_2016']               v5.5  Cre01.g000200   \n",
       "6                ['Bajhaiya_2016']               v5.5  Cre01.g000250   \n",
       "7                ['Bajhaiya_2016']               v5.5  Cre01.g000300   \n",
       "8  ['Hemme_2014', 'Bajhaiya_2016']               v5.5  Cre01.g000350   \n",
       "9                ['Bajhaiya_2016']               v5.5  Cre01.g000400   \n",
       "\n",
       "  gene_symbol                                         pathway_id  \\\n",
       "0         NaN                                                NaN   \n",
       "1         NaN                                                NaN   \n",
       "2         NaN                                                NaN   \n",
       "3         NaN                                                NaN   \n",
       "4        ZRT2                                                NaN   \n",
       "5         NaN                                                NaN   \n",
       "6         NaN                                                NaN   \n",
       "7         NaN  ['Creinhardtii PWY-5667', 'Creinhardtii PWY-74...   \n",
       "8         NaN                                                NaN   \n",
       "9         NaN                                                NaN   \n",
       "\n",
       "  transcript_id_v5.3.1  \n",
       "0                g2.t1  \n",
       "1                g3.t1  \n",
       "2   Cre01.g000050.t1.3  \n",
       "3   Cre01.g000100.t1.3  \n",
       "4   Cre01.g000150.t1.2  \n",
       "5   Cre01.g000200.t1.3  \n",
       "6   Cre01.g000250.t1.2  \n",
       "7   Cre01.g000300.t1.3  \n",
       "8   Cre01.g000350.t1.3  \n",
       "9   Cre01.g000400.t1.2  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryextract(i, pattern):\n",
    "    \n",
    "    '''This function takes in a string and returns the first matching group in the search pattern'''\n",
    "    \n",
    "    try: \n",
    "        m = re.search(pattern, i).group(1)\n",
    "        return(m)\n",
    "    \n",
    "    except AttributeError:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import gff as working dataframe. Extract Name and PAC id from the column 'attribute' as separate columns\n",
    "with gzip.open(gff_file_path, \"rt\", encoding=\"utf-8\") as z:\n",
    "    \n",
    "    df = pd.read_csv(z,delimiter=r\"\\s+\",skiprows=1,header=None)\n",
    "    df.columns = ['chromosome', 'source', 'feature', 'start', 'end', 'score', 'strand', 'phase', 'attributes']\n",
    "    df['Name'] = df.attributes.apply(lambda x: tryextract(x, r\"Name=(.+);pacid\"))\n",
    "    df['ID'] = df.attributes.apply(lambda x: tryextract(x, r\"ID=(PAC:[0-9]+)\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[df.feature ==\"mRNA\"].sort_values(by=\"Name\").reset_index()\n",
    "search_list = list(subset.Name)\n",
    "pac_id = list(np.unique(df.ID.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 10.597784519195557 s\n"
     ]
    }
   ],
   "source": [
    "#Order list of matching PAC id's to 'transcript_id_v5.3.1'\n",
    "#import re\n",
    "t0 = time.time()\n",
    "pacid_list = []\n",
    "\n",
    "for row in range(0,len(merged)):\n",
    "    transcript_id = merged.loc[row,'transcript_id_v5.3.1']\n",
    "    pacid = None\n",
    "    \n",
    "    index = sag.BinarySearch(search_list, transcript_id)\n",
    "    \n",
    "    if index > -1: \n",
    "        pacid = subset.loc[index, \"ID\"]\n",
    "                    \n",
    "    pacid_list.append(pacid)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time taken\", t1-t0, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['PAC_id'] = pacid_list\n",
    "#master_gene_sfs = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208.23728322982788\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "exon_positions = []\n",
    "features_condition = ['five_prime_UTR', 'CDS', 'three_prime_UTR']\n",
    "#keep gff rows that contain CDS positions, remove gff rows that are missing PAC_ID's\n",
    "filtered_df = df[df.feature.isin(features_condition)].dropna(subset = ['ID'])\n",
    "#pos =0\n",
    "#loop through sampled transcipts with PAC_ID's; transcript filtered by condition variable\n",
    "for pac_index in range(0,len(pac_id)): \n",
    "    \n",
    "    temp = filtered_df[filtered_df.ID==pac_id[pac_index]]\n",
    "    #print(transcript)\n",
    "    if len(temp)== 0: exon_positions.append(None)\n",
    "        \n",
    "    else:\n",
    "        exon_positions.append([])\n",
    "        #add CDS coordinates as tuple (chromosome, start, end) to gene_set\n",
    "        for i in list(temp.index):\n",
    "            exon_positions[pac_index].append((temp.chromosome[i], temp.start[i], temp.end[i]))\n",
    "    #pos+=1\n",
    "    #if count>20: break\n",
    "        \n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exon_positions.pickle', 'wb') as f:\n",
    "    pickle.dump(exon_positions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "exon_positions = pickle.load(open(\"exon_positions.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_SFS(SFS_dict, max_alleles = 18):\n",
    "    \n",
    "    '''This function aggregates SFS vectors from SFSs_from_annotation() dictionary output into one SFS vector.\n",
    "    Default max_alleles = 18'''\n",
    "    \n",
    "    ref_SFS = range(0,max_alleles)\n",
    "    new_sfs = [0]*max_alleles\n",
    "    for key in SFS_dict.keys():\n",
    "        \n",
    "        sfs_length = len(SFS_dict[key].sfs)\n",
    "        #print(key)\n",
    "        normalized_index = [round(i/sfs_length*max_alleles)-1 for i in range(1,sfs_length+1)]\n",
    "        #print(normalized_index)\n",
    "        for i in range(0,sfs_length): new_sfs[normalized_index[i]]+=SFS_dict[key].sfs[i]\n",
    "        \n",
    "    return(new_sfs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Test function logic\n",
    "max_alleles =10\n",
    "SFS_dict = {10: [1,0,1,0,1,0,1,0,1,0],\n",
    "           5: [1,0,1,0,1]}\n",
    "expected_output = [1,1,1,0,1,1,1,0,1,1]\n",
    "ref_SFS = range(0,max_alleles)\n",
    "new_sfs = [0]*max_alleles\n",
    "\n",
    "for key in SFS_dict.keys():\n",
    "        \n",
    "        sfs_length = len(SFS_dict[key])\n",
    "        #print(key)\n",
    "        normalized_index = [round(i/sfs_length*max_alleles)-1 for i in range(1,sfs_length+1)]\n",
    "        #print(normalized_index)\n",
    "        for i in range(0,sfs_length): new_sfs[normalized_index[i]]+=SFS_dict[key][i]\n",
    "\n",
    "if new_sfs==expected_output: print(\"reduce_SFS() logic worked as expected\")\n",
    "else: print(\"reduce_SFS() logic did not work as expected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up input for testing collapse_SFS()\n",
    "#coordinates = exon_positions[4]\n",
    "coordinates = [('chromosome_8', 2509558, 2509598), \n",
    "               ('chromosome_8', 2509599, 2509853), \n",
    "               ('chromosome_8', 2510289, 2510399), \n",
    "               ('chromosome_8', 2510839, 2510925), \n",
    "               ('chromosome_8', 2511587, 2511649), \n",
    "               ('chromosome_8', 2511975, 2512075), \n",
    "               ('chromosome_8', 2512492, 2512594), \n",
    "               ('chromosome_8', 2512961, 2513116), \n",
    "               ('chromosome_8', 2513372, 2513452), \n",
    "               ('chromosome_8', 2513453, 2514419)]\n",
    "neutral = SFSs_from_annotation(annotation_tabix, coordinates, min_alleles=12, neutral_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test if collapse_SFS() returns error\n",
    "reduce_SFS(neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFS vectors from SFS dictionary output of `SFSs_from_annotation()` contains the number of invariants as the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in neutral.keys():\n",
    "    print (\"SFS vector\", neutral[key].sfs)\n",
    "    print(\"Number of alleles (also length of SFS vector):\", neutral[key].alleles)\n",
    "    print(\"Number of invariant sites:\", neutral[key].invariant())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 genes took: 18.34585189819336 seconds.\n",
      "Estimated calculation time: 59.712690286636345 minutes\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "selected_SFS = []\n",
    "neutral_SFS = []\n",
    "#for i in range(len(exon_positions)):\n",
    "for i in range(0,100):\n",
    "    coordinates = exon_positions[i]\n",
    "    if coordinates != None:\n",
    "        selected = SFSs_from_annotation(coordinates, min_alleles=12, neutral_only=False)\n",
    "        selected_SFS.append(reduce_SFS(selected, max_alleles = 18))\n",
    "        neutral = SFSs_from_annotation(coordinates, min_alleles=12, neutral_only=True)\n",
    "        neutral_SFS.append(reduce_SFS(neutral, max_alleles = 18))\n",
    "t1 = time.time()\n",
    "print(\"100 genes took:\", t1-t0, \"seconds.\")\n",
    "print(\"Estimated calculation time:\", len(exon_positions)/100*(t1-t0)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "t0 = time.time()\n",
    "selected_SFS = []\n",
    "neutral_SFS = []\n",
    "def collect_reduced_SFS(exon_positions, start, end):\n",
    "#for i in range(len(exon_positions)):\n",
    "    for i in range(start,end):\n",
    "        coordinates = exon_positions[i]\n",
    "        if coordinates != None:\n",
    "            selected = SFSs_from_annotation(annotation_tabix, coordinates, min_alleles=12, neutral_only=False)\n",
    "            selected_SFS.append(collapse_SFS(selected, max_alleles = 18))\n",
    "            neutral = SFSs_from_annotation(annotation_tabix, coordinates, min_alleles=12, neutral_only=True)\n",
    "            neutral_SFS.append(collapse_SFS(neutral, max_alleles = 18))\n",
    "t1 = time.time()\n",
    "#print(\"100 genes took:\", t1-t0, \"seconds.\")\n",
    "print(\"Estimated calculation time:\", len(exon_positions)/100*(t1-t0)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t0 = time.time()\n",
    "def collect_reduced_SFS(exon_positions):\n",
    "#for i in range(len(exon_positions)):\n",
    "    coordinates = exon_positions\n",
    "    if coordinates != None:\n",
    "        try:\n",
    "            selected = SFSs_from_annotation(annotation_tabix, coordinates, min_alleles=12, neutral_only=False)\n",
    "            selected_SFS = reduce_SFS(selected, max_alleles = 18)\n",
    "        except ValueError: selected_SFS = None\n",
    "        try: \n",
    "            neutral = SFSs_from_annotation(annotation_tabix, coordinates, min_alleles=12, neutral_only=True)\n",
    "            neutral_SFS = reduce_SFS(neutral, max_alleles = 18)\n",
    "        except ValueError: neutral_SFS =  None\n",
    "        \n",
    "        #return coordinates, selected_SFS, neutral_SFS\n",
    "        return selected_SFS, neutral_SFS\n",
    "#t1 = time.time()\n",
    "#print(\"100 genes took:\", t1-t0, \"seconds.\")\n",
    "#print(\"Estimated calculation time:\", len(exon_positions)/100*(t1-t0)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([227, 0, 7, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([820, 39, 9, 1, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [126, 9, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([2115, 62, 40, 28, 13, 19, 13, 14, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0], [273, 12, 11, 3, 5, 6, 6, 3, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([4233, 85, 33, 18, 11, 14, 10, 6, 13, 1, 0, 0, 0, 0, 0, 0, 0, 0], [791, 38, 16, 8, 8, 8, 8, 4, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([1337, 32, 18, 3, 1, 13, 5, 10, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [127, 4, 4, 1, 1, 6, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([2329, 40, 27, 13, 33, 12, 14, 3, 11, 2, 0, 0, 0, 0, 0, 0, 0, 0], [228, 5, 3, 2, 3, 2, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([3870, 136, 42, 39, 19, 36, 19, 26, 12, 4, 0, 0, 0, 0, 0, 0, 0, 0], [549, 31, 14, 16, 6, 13, 9, 5, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([3847, 136, 42, 39, 19, 36, 19, 26, 12, 3, 0, 0, 0, 0, 0, 0, 0, 0], [526, 31, 14, 16, 6, 13, 9, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "([1448, 16, 23, 16, 4, 17, 8, 8, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [263, 3, 7, 6, 2, 11, 4, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#test collect_reduced_SFS\n",
    "for i in range(10):\n",
    "    print(collect_reduced_SFS(exon_positions[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('/home/chenwe72/local/lib/python3.5/site-packages/')\n",
    "import psutil #psutil-5.7.0\n",
    "from multiprocessing import Pool #multiprocessing-2.6.2.1\n",
    "import time\n",
    "# logical=True counts threads, but we are interested in cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psutil.cpu_count(logical=False))\n",
    "pool = Pool(psutil.cpu_count(logical=False))\n",
    "print(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.59 µs ± 18.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "[10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56]\n"
     ]
    }
   ],
   "source": [
    "variable = 5\n",
    "def add_var(input):\n",
    "    output = input+variable\n",
    "    return output\n",
    "def test(input):\n",
    "    output = add_var(input)*2\n",
    "    return output\n",
    "%timeit list(map(test, range(24)))\n",
    "print(list(map(test, range(24))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exon_positions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([227, 0, 7, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([820, 39, 9, 1, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [126, 9, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([2115, 62, 40, 28, 13, 19, 13, 14, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0], [273, 12, 11, 3, 5, 6, 6, 3, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([4233, 85, 33, 18, 11, 14, 10, 6, 13, 1, 0, 0, 0, 0, 0, 0, 0, 0], [791, 38, 16, 8, 8, 8, 8, 4, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0]), ([1337, 32, 18, 3, 1, 13, 5, 10, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [127, 4, 4, 1, 1, 6, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(list(map(collect_reduced_SFS, exon_positions[:5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56]\n",
      "[([820, 39, 9, 1, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [126, 9, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=6) as pool: \n",
    "    print(list(pool.map(test, range(24))))\n",
    "    \n",
    "with Pool(processes=1) as pool: \n",
    "    print(list(pool.map(collect_reduced_SFS, exon_positions[1:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([820, 39, 9, 1, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [126, 9, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([2115, 62, 40, 28, 13, 19, 13, 14, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0], [273, 12, 11, 3, 5, 6, 6, 3, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([4233, 85, 33, 18, 11, 14, 10, 6, 13, 1, 0, 0, 0, 0, 0, 0, 0, 0], [791, 38, 16, 8, 8, 8, 8, 4, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0]), ([1337, 32, 18, 3, 1, 13, 5, 10, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [127, 4, 4, 1, 1, 6, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([2329, 40, 27, 13, 33, 12, 14, 3, 11, 2, 0, 0, 0, 0, 0, 0, 0, 0], [228, 5, 3, 2, 3, 2, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0]), ([3870, 136, 42, 39, 19, 36, 19, 26, 12, 4, 0, 0, 0, 0, 0, 0, 0, 0], [549, 31, 14, 16, 6, 13, 9, 5, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]), ([3847, 136, 42, 39, 19, 36, 19, 26, 12, 3, 0, 0, 0, 0, 0, 0, 0, 0], [526, 31, 14, 16, 6, 13, 9, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), ([1448, 16, 23, 16, 4, 17, 8, 8, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [263, 3, 7, 6, 2, 11, 4, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=1) as pool: \n",
    "    print(list(pool.map(collect_reduced_SFS, exon_positions[1:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([820, 39, 9, 1, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [126, 9, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "with Pool(processes=2) as pool: \n",
    "    print(list(pool.map(collect_reduced_SFS, exon_positions[1:2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=2) as pool: \n",
    "    print(list(pool.map(collect_reduced_SFS, exon_positions[1:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188, 7, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "    \n",
    "def main():\n",
    "    pool = Pool(processes=1)\n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, neutral_only=False)\n",
    "    for i in list(pool.map(func, exon_positions[0:1])):\n",
    "        for key, value in i.items():\n",
    "            print(value.sfs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188, 7, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[24, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[774, 36, 9, 1, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[169, 4, 9, 4, 0, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[387, 16, 6, 4, 5, 5, 1, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1156, 31, 17, 12, 4, 6, 4, 7, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[68, 2, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[140, 4, 3, 0, 5, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[195, 7, 6, 3, 2, 3, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pool = Pool(processes=1)\n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, neutral_only=False)\n",
    "    for i in list(pool.map(func, exon_positions[0:3])):\n",
    "        for key, value in i.items():\n",
    "            print(value.sfs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188, 7, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[39, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[24, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[774, 36, 9, 1, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[169, 4, 9, 4, 0, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[387, 16, 6, 4, 5, 5, 1, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1156, 31, 17, 12, 4, 6, 4, 7, 7, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[68, 2, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[140, 4, 3, 0, 5, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[195, 7, 6, 3, 2, 3, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pool = Pool(processes=2)\n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, neutral_only=False)\n",
    "    for i in list(pool.map(func, exon_positions[0:3])):\n",
    "        for key, value in i.items():\n",
    "            print(value.sfs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[227, 0, 7, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pool = Pool(processes=1)\n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, neutral_only=False)\n",
    "    selected_vector = list(pool.map(func, exon_positions[0:1]))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    print(list(pool.map(func, sfs_vectors)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[227, 0, 7, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [820, 39, 9, 1, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2115, 62, 40, 28, 13, 19, 13, 14, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    pool = Pool(processes=1)\n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, neutral_only=False)\n",
    "    sfs_vectors = list(pool.map(func, exon_positions[0:3]))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    print(list(pool.map(func, sfs_vectors)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "def main():\n",
    "    pool = Pool(processes=4)\n",
    "    \n",
    "    exon_portion = exon_positions[0:100]\n",
    "    \n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, neutral_only=False)\n",
    "    selected = list(pool.map(func, exon_portion))\n",
    "    \n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, neutral_only=True)\n",
    "    neutral = list(pool.map(func, exon_portion))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    reduced_selected = list(pool.map(func, selected))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    reduced_neutral = list(pool.map(func, neutral))\n",
    "    \n",
    "    return reduced_selected, reduced_neutral\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    reduced_selected, reduced_neutral = main()\n",
    "    \n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.720616340637207\n",
      "[[227, 0, 7, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [820, 39, 9, 1, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2115, 62, 40, 28, 13, 19, 13, 14, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4233, 85, 33, 18, 11, 14, 10, 6, 13, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1337, 32, 18, 3, 1, 13, 5, 10, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2329, 40, 27, 13, 33, 12, 14, 3, 11, 2, 0, 0, 0, 0, 0, 0, 0, 0], [3870, 136, 42, 39, 19, 36, 19, 26, 12, 4, 0, 0, 0, 0, 0, 0, 0, 0], [3847, 136, 42, 39, 19, 36, 19, 26, 12, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1448, 16, 23, 16, 4, 17, 8, 8, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [2518, 58, 36, 16, 19, 35, 21, 28, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0], [352, 4, 4, 1, 1, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [711, 16, 23, 23, 6, 14, 6, 4, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1635, 38, 12, 3, 27, 19, 14, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2644, 70, 30, 38, 22, 19, 14, 8, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2563, 67, 30, 36, 22, 19, 14, 8, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1835, 9, 3, 8, 12, 13, 0, 1, 9, 8, 0, 0, 0, 0, 0, 0, 0, 0], [2978, 75, 30, 42, 32, 28, 12, 20, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0], [524, 10, 11, 1, 6, 1, 3, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [933, 9, 20, 15, 7, 19, 10, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2684, 22, 11, 12, 4, 19, 18, 13, 7, 5, 0, 0, 0, 0, 0, 0, 0, 0], [2603, 65, 23, 6, 7, 12, 15, 19, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4432, 42, 13, 30, 30, 15, 3, 2, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [2360, 39, 19, 17, 7, 18, 17, 22, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0], [2360, 39, 19, 17, 7, 18, 17, 22, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0], [500, 7, 6, 5, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2490, 32, 27, 25, 7, 6, 1, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2112, 46, 13, 19, 6, 12, 9, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2597, 85, 33, 31, 31, 18, 21, 19, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4886, 72, 7, 4, 2, 13, 70, 115, 49, 10, 0, 0, 0, 0, 0, 0, 0, 0], [1286, 27, 20, 12, 18, 13, 18, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1935, 56, 16, 10, 4, 11, 5, 17, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0], [1228, 34, 27, 8, 6, 12, 7, 4, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3450, 120, 27, 23, 36, 26, 25, 31, 15, 2, 0, 0, 0, 0, 0, 0, 0, 0], [859, 79, 28, 9, 5, 8, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [840, 3, 13, 4, 1, 10, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6799, 56, 20, 13, 12, 52, 143, 9, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3152, 120, 53, 33, 63, 52, 43, 13, 28, 4, 0, 0, 0, 0, 0, 0, 0, 0], [4383, 84, 31, 76, 35, 17, 39, 70, 31, 3, 0, 0, 0, 0, 0, 0, 0, 0], [1582, 29, 23, 18, 14, 11, 2, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1097, 25, 53, 11, 15, 12, 6, 16, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1088, 25, 53, 11, 15, 12, 6, 16, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [993, 10, 12, 9, 2, 13, 13, 7, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], [2740, 43, 64, 30, 29, 21, 21, 5, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0], [1516, 42, 12, 25, 23, 24, 14, 5, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0], [1352, 66, 27, 38, 11, 9, 19, 12, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0], [1511, 11, 4, 23, 6, 8, 5, 3, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0], [4744, 135, 83, 111, 76, 49, 25, 30, 16, 4, 0, 0, 0, 0, 0, 0, 0, 0], [2505, 103, 49, 26, 15, 9, 7, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3207, 42, 35, 30, 22, 10, 31, 49, 24, 1, 0, 0, 0, 0, 0, 0, 0, 0], [2469, 65, 1, 0, 1, 1, 9, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1472, 30, 46, 14, 4, 7, 8, 7, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4321, 96, 48, 32, 16, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1465, 25, 11, 4, 10, 2, 13, 19, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [410, 14, 3, 4, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1500, 55, 17, 13, 3, 19, 15, 7, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3903, 77, 74, 34, 11, 14, 16, 17, 15, 5, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2087, 14, 0, 16, 18, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1706, 24, 9, 8, 8, 6, 6, 8, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0], [679, 20, 10, 10, 7, 8, 3, 3, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0], [1554, 11, 2, 0, 16, 21, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1293, 9, 2, 0, 12, 19, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2831, 66, 34, 41, 45, 48, 35, 27, 23, 8, 0, 0, 0, 0, 0, 0, 0, 0], [3164, 54, 69, 28, 33, 22, 10, 9, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1927, 61, 55, 15, 6, 6, 9, 9, 13, 4, 0, 0, 0, 0, 0, 0, 0, 0], [133, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3358, 86, 84, 27, 15, 14, 9, 9, 5, 11, 0, 0, 0, 0, 0, 0, 0, 0], [217, 4, 13, 4, 3, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4603, 76, 41, 13, 11, 30, 20, 13, 9, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1317, 45, 13, 18, 8, 2, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1037, 42, 24, 5, 6, 6, 5, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1621, 66, 25, 23, 11, 17, 9, 3, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0], [857, 4, 2, 3, 11, 3, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2572, 47, 29, 30, 32, 23, 35, 8, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1245, 43, 37, 30, 17, 18, 11, 12, 11, 3, 0, 0, 0, 0, 0, 0, 0, 0], [2448, 13, 61, 40, 13, 34, 8, 33, 26, 2, 0, 0, 0, 0, 0, 0, 0, 0], [1657, 41, 26, 23, 22, 13, 9, 16, 13, 2, 0, 0, 0, 0, 0, 0, 0, 0], [3330, 73, 47, 37, 24, 21, 16, 13, 23, 1, 0, 0, 0, 0, 0, 0, 0, 0], [466, 7, 8, 17, 2, 4, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [461, 7, 8, 17, 2, 4, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1338, 93, 32, 25, 22, 18, 17, 8, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0], [963, 16, 0, 39, 7, 0, 0, 7, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [2112, 96, 28, 12, 4, 15, 24, 27, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0], [3185, 62, 60, 32, 12, 23, 15, 19, 11, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2145, 67, 34, 39, 27, 23, 20, 4, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0], [2142, 67, 34, 39, 27, 23, 20, 4, 20, 2, 0, 0, 0, 0, 0, 0, 0, 0], [2185, 12, 8, 14, 22, 9, 3, 10, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0], [1238, 29, 7, 12, 5, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1626, 37, 33, 9, 2, 1, 10, 3, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2097, 10, 1, 17, 8, 4, 29, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2478, 65, 30, 29, 13, 9, 15, 14, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(t1-t0)\n",
    "print(reduced_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SFSs_from_annotation(coordinates, min_alleles=None, neutral_only=False): \n",
    "    annotation_tabix = TabixFile(filename=\"/scratch/research/references/chlamydomonas/5.3_chlamy_w_organelles_mt_minus/annotation/concatenated_GFF/annotation_table.txt.gz\")\n",
    "    \"\"\"\n",
    "    This function will return a dictionary of SFS objects\n",
    "    The dictionary will contain one SFS for each number of alleles that can be called\n",
    "        ie min_alleles to total number of individuals sequenced\n",
    "    It is possible to combine these SFSs by:\n",
    "        rounding MAF * (number of individuals sequenced) and keeping only one SFS\n",
    "    Arguments:\n",
    "     - take a TabixFile of the annotation table\n",
    "     - the chromosome, start and end (1-based inclusive)\n",
    "     - an optional minimum number of alleles - below this the site is shiite so don't take a bite\n",
    "     - neutral_only skips sites that aren't intergenic, intronic or 4-fold degenerate\n",
    "    \"\"\"\n",
    "    SFSs = {}\n",
    "    \n",
    "    #Loop through input list of (chromosome, start, end) from gff3\n",
    "    for i in coordinates:\n",
    "        chromosome, start, end = i\n",
    "        for line in annotation_tabix.fetch(chromosome, start-1, end):\n",
    "        # `annotation_line` is a class that has all the annotation table columns as attributes \n",
    "            a = annotation_table.annotation_line(line) #a has a lot of attributes\n",
    "            allele_counts = a.quebec_alleles\n",
    "            if neutral_only and sum([int(i) for i in [a.intergenic, a.intronic, a.fold4]]) == 0: #these are all neutral/silent sites\n",
    "                #because we are only looking at CDS, we will never get intergenic and intronic sequences\n",
    "                #our SFS in neutral  is a fold4 site\n",
    "                #fold0 sites go in selected SFS vector in est_dfe\n",
    "                continue\n",
    "            try:\n",
    "                MAF, total_alleles_called  = MAF_from_allele_count(allele_counts,min_alleles=min_alleles)\n",
    "                #if MAF > 0: print(MAF, total_alleles_called)\n",
    "            except TypeError: \n",
    "                continue\n",
    "            if min_alleles != None and total_alleles_called < min_alleles: #filter sites with too few alleles called\n",
    "                continue\n",
    "            if total_alleles_called not in SFSs: \n",
    "                #make SFS dictionary where key = n for SFS vector of length n\n",
    "                #you can't feed program with different alleles, so try to standardize and round them somehow in proportion to max alleles\n",
    "                SFSs[total_alleles_called] = SFS([0]*(total_alleles_called+1))\n",
    "            SFSs[total_alleles_called].add(MAF,total_alleles_called)\n",
    "\n",
    "\n",
    "    return SFSs\n",
    "\n",
    "\n",
    "def MAF_from_allele_count(allele_counts, min_alleles=None): # num of rare alleles/num of total alleles\n",
    "    \"\"\"\n",
    "    return the minor allele frequency and the number of called alleles\n",
    "    take a single allele_counts from annotation table ie, A:C:G:T    \n",
    "    optionally min_alleles will filter sites with too few alleles called\n",
    "    \"\"\"\n",
    "    minor_allele_count = sorted([int(i) for i in allele_counts.split(\":\")])[-2]\n",
    "    total_alleles_called = sum([int(i) for i in allele_counts.split(\":\")])\n",
    "    if min_alleles != None and total_alleles_called <= min_alleles:\n",
    "        return None\n",
    "    try:\n",
    "        MAF = minor_allele_count/float(total_alleles_called)\n",
    "        return (MAF,total_alleles_called)\n",
    "    except ZeroDivisionError:\n",
    "        return None\n",
    "    \n",
    "\n",
    "##annotation_table has clones filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%bash is a cell magic, but the cell body is empty.\n"
     ]
    }
   ],
   "source": [
    "%%bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##chromosome=sequence from Chlamydomonas reinhardtii v5.3 genome or mtMinus, mtDNA or cpDNA [STRING]\n",
      "##position=1-based position in on the chromosome [INTEGER] \n",
      "##reference_base=nucleotide carried by Chlamydomonas reinhardtii v5.3 genome at that position and chromosome [STRING]\n",
      "##genic=position is part of a gene (UTR, intron, exon, CDS) [0/1]\n",
      "##exonic=position is part of an exon (UTR, CDS)[0/1]\n",
      "##intronic=position is part of an intron (inferred from positions of exons)[0/1]\n",
      "##intergenic=position is not annotated as genic[0/1]\n",
      "##utr5=position is 5' UTR[0/1]\n",
      "##utr3=position is 3' UTR[0/1]\n",
      "##fold0=position is 0-fold degenerate [0/1]\n",
      "##fold4=position is 4-fold degenerate [0/1]\n",
      "##fold2=position is 2-fold degenerate [0/1]\n",
      "##fold3=position is 3-fold degenerate [0/1]\n",
      "##CDS=position is protein coding [0/1]\n",
      "##mRNA=position is part of the transcribed region. This unfortunately includes the introns[0/1]\n",
      "##rRNA=an annotated ribosomal RNA[0/1]\n",
      "##tRNA=an annoated tranfer RNA [0/1]\n",
      "##feature_names=A python style list of the names of features overlapping this position [LIST]\n",
      "##feature_types=A python style list of the feature types that overlap this position [LIST]\n",
      "##reference=redundant column with reference[0/1]\n",
      "##feature_ID=the unique transscript ID used to define this CDS transcript (usually ness_ID) [STRING]\n",
      "##cds_position=0-based postion of this base in the CDS adjusted for strand [INTEGER]\n",
      "##strand=the strand on which the CDS is encoded [+/-]\n",
      "##frame= the reading frame of the position in the CDS [0/1/2]\n",
      "##codon= the codon that this position is part of [STRING]\n",
      "##aa=The amino acid encoded by this protein [0/1]\n",
      "##degen=the coding degeneracy of this codon position [0/2/3/4]\n",
      "##FPKM=mean expression [FLOAT]\n",
      "##rho=mean population recombination rate (rho=Ner) [FLOAT]\n",
      "##FAIRE=height of FAIRE peak  indicating nucleosome depletion. Data from from Vischi-Winck et al 2013 [FLOAT]\n",
      "##recombination=rate of recombination measured from JGI Grimwood map data. bp/cM [FLOAT]\n",
      "##mutability=predicted probability of mutation from Ness et al 2015 Genome Research [FLOAT]\n",
      "##all_quebec_alleles=number of A:C:G:T calls with DP>3 and GQ>20 in the all_quebec sample [INTEGER:INTEGER:INTEGER:INTEGER]\n",
      "#chromosome\tposition\treference_base\tgenic\texonic\tintronic\tintergenic\tutr5\tutr3\tfold0\tfold4\tfold2\tfold3\tCDS\tmRNA\trRNA\ttRNA\tfeature_names\tfeature_types\tfeature_ID\tcds_position\tstrand\tframe\tcodon\taa\tdegen\tFPKM\trho\tFAIRE\trecombination\tmutability\tall_quebec_alleles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tabix: invalid option -- '-'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tabix --H /scratch/research/references/chlamydomonas/5.3_chlamy_w_organelles_mt_minus/annotation/concatenated_GFF/annotation_table.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-203-4fa6e0f2d284>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-203-4fa6e0f2d284>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    for i in pool.map(SFSs_from_annotation, (annotation_tabix, exon_positions[:10], min_alleles=12, neutral_only=False)) :\u001b[0m\n\u001b[0m                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append('../../scripts/')\n",
    "    from ness_vcf import SFS\n",
    "    from pysam import TabixFile\n",
    "    from annotation import annotation_table\n",
    "    import alice_custom as ac\n",
    "    import pickle\n",
    "\n",
    "    annotation_tabix = TabixFile(filename=\"/scratch/research/references/chlamydomonas/5.3_chlamy_w_organelles_mt_minus/annotation/concatenated_GFF/annotation_table.txt.gz\")\n",
    "    exon_positions = pickle.load(open(\"exon_positions.pickle\", \"rb\"))\n",
    "    \n",
    "    selected_SFS = []\n",
    "    neutral_SFS = []\n",
    "    \n",
    "    with Pool(processes=6) as pool:\n",
    "        for i in pool.map(SFSs_from_annotation, (annotation_tabix, exon_positions[:10], min_alleles=12, neutral_only=False)) :\n",
    "            #pos, selected, neutral = i\n",
    "            #selected_SFS.append(selected)\n",
    "            #neutral_SFS.append(neutral)\n",
    "            print(i)\n",
    "\n",
    "#print(selected_SFS)\n",
    "#with open('selected_SFS.pickle', 'wb') as f:\n",
    "#    pickle.dump(selected_SFS, f)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    with Pool(processes=3) as pool:\n",
    "#        results = pool.map_async(collect_reduced_SFS, exon_positions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
