{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate neutral and selected SFS vectors of all transcripts (=gene)\n",
    "\n",
    "#### Method:\n",
    "\n",
    "1. Get all unique PAC id's from ggf3 file `/scratch/research/references/chlamydomonas/5.3_chlamy_w_organelles_mt_minus/annotation/Creinhardtii_v5.3_223_gene.gff3.gz`\n",
    "    - Each PAC id is associated with a unique transcript.\n",
    "2. Get chromosome, start, and end positions of each `five_prime_UTR`, `CDS`, and `three_prime_UTR` in each PAC id. Store as list `exon_positions` and export to working folder.\n",
    "    - `exon_positions`: `<list>` \n",
    "    \n",
    "       `[[(chromosome <str>, start <int>, end <str>),...,(chromosome <str>, start <int>, end <str>)], ...,`\n",
    "       \n",
    "       `[(chromosome <str>, start <int>, end <str>),...,(chromosome <str>, start <int>, end <str>)]]`\n",
    "3. Pass `exon_positions` through `SFSs_from_annotation()` to get SFS vectors for each transcript for neutral and selected sites.\n",
    "4. Pass the dictionary output of SFS vectors from `SFSs_from_annotation()` through `reduce_SFS()` to reduce SFS vectors into one SFS vector. Store all reduced vector outputs as lists of vectors -- `selected_SFS` and `neutral_SFS`.\n",
    "5. Create dataframe of reduced SFS for each transcript.\n",
    "\n",
    "| PAC_id  \t| selected_SFS        \t| neutral_SFS         \t|\n",
    "|---------\t|---------------------\t|---------------------\t|\n",
    "| `<str>` \t| `[<int>,...,<int>]` \t| `[<int>,...,<int>]` \t|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../scripts/')\n",
    "#sys.path\n",
    "import pandas as pd\n",
    "import Search_algorithms as sag\n",
    "import gzip\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from functools import partial\n",
    "import psutil #psutil-5.7.0\n",
    "from multiprocessing import Pool #multiprocessing-2.6.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get all unique PAC id's from ggf3 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gff_file_path='/scratch/research/references/chlamydomonas/5.3_chlamy_w_organelles_mt_minus/annotation/Creinhardtii_v5.3_223_gene.gff3.gz'\n",
    "merged = pd.read_csv(\"../../data/intermediate_data_02/sampled_genes.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_detected</th>\n",
       "      <th>num_manipulated</th>\n",
       "      <th>num_sampled</th>\n",
       "      <th>source</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>annotation_version</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>gene_symbol</th>\n",
       "      <th>pathway_id</th>\n",
       "      <th>transcript_id_v5.3.1</th>\n",
       "      <th>PAC_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bajhaiya_2016</td>\n",
       "      <td>Cre01.g000017.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2.t1</td>\n",
       "      <td>PAC:26903746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Kwak_2017</td>\n",
       "      <td>Cre01.g000017.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2.t1</td>\n",
       "      <td>PAC:26903746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bajhaiya_2016</td>\n",
       "      <td>Cre01.g000033.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g3.t1</td>\n",
       "      <td>PAC:26903463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Gargouri_2015</td>\n",
       "      <td>Cre01.g000050.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000050.t1.3</td>\n",
       "      <td>PAC:26903339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bajhaiya_2016</td>\n",
       "      <td>Cre01.g000100.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000100.t1.3</td>\n",
       "      <td>PAC:26903974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_detected  num_manipulated  num_sampled         source  \\\n",
       "0           0.0                0          1.0  Bajhaiya_2016   \n",
       "1           0.0                0          1.0      Kwak_2017   \n",
       "2           1.0                0          1.0  Bajhaiya_2016   \n",
       "3           0.0                0          1.0  Gargouri_2015   \n",
       "4           0.0                0          1.0  Bajhaiya_2016   \n",
       "\n",
       "        transcript_id annotation_version        gene_id gene_symbol  \\\n",
       "0  Cre01.g000017.t1.1               v5.5  Cre01.g000017         NaN   \n",
       "1  Cre01.g000017.t1.1               v5.5  Cre01.g000017         NaN   \n",
       "2  Cre01.g000033.t1.1               v5.5  Cre01.g000033         NaN   \n",
       "3  Cre01.g000050.t1.1               v5.5  Cre01.g000050         NaN   \n",
       "4  Cre01.g000100.t1.1               v5.5  Cre01.g000100         NaN   \n",
       "\n",
       "  pathway_id transcript_id_v5.3.1        PAC_id  \n",
       "0        NaN                g2.t1  PAC:26903746  \n",
       "1        NaN                g2.t1  PAC:26903746  \n",
       "2        NaN                g3.t1  PAC:26903463  \n",
       "3        NaN   Cre01.g000050.t1.3  PAC:26903339  \n",
       "4        NaN   Cre01.g000100.t1.3  PAC:26903974  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryextract(i, pattern):\n",
    "    \n",
    "    '''This function takes in a string and returns the first matching group in the search pattern'''\n",
    "    \n",
    "    try: \n",
    "        m = re.search(pattern, i).group(1)\n",
    "        return(m)\n",
    "    \n",
    "    except AttributeError:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import gff as working dataframe. Extract Name and PAC id from the column 'attribute' as separate columns\n",
    "with gzip.open(gff_file_path, \"rt\", encoding=\"utf-8\") as z:\n",
    "    \n",
    "    df = pd.read_csv(z,delimiter=r\"\\s+\",skiprows=1,header=None)\n",
    "    df.columns = ['chromosome', 'source', 'feature', 'start', 'end', 'score', 'strand', 'phase', 'attributes']\n",
    "    df['Name'] = df.attributes.apply(lambda x: tryextract(x, r\"Name=(.+);pacid\"))\n",
    "    df['ID'] = df.attributes.apply(lambda x: tryextract(x, r\"ID=(PAC:[0-9]+)\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_id = list(np.unique(df.ID.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromosome</th>\n",
       "      <th>source</th>\n",
       "      <th>feature</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>score</th>\n",
       "      <th>strand</th>\n",
       "      <th>phase</th>\n",
       "      <th>attributes</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chromosome_1</td>\n",
       "      <td>phytozome8_0</td>\n",
       "      <td>gene</td>\n",
       "      <td>24026</td>\n",
       "      <td>30617</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>ID=Cre01.g000050;Name=Cre01.g000050</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chromosome_1</td>\n",
       "      <td>phytozome8_0</td>\n",
       "      <td>mRNA</td>\n",
       "      <td>24026</td>\n",
       "      <td>30617</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>ID=PAC:26903339;Name=Cre01.g000050.t1.3;pacid=...</td>\n",
       "      <td>Cre01.g000050.t1.3</td>\n",
       "      <td>PAC:26903339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chromosome_1</td>\n",
       "      <td>phytozome8_0</td>\n",
       "      <td>five_prime_UTR</td>\n",
       "      <td>24026</td>\n",
       "      <td>24125</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>.</td>\n",
       "      <td>ID=PAC:26903339.five_prime_UTR.1;Parent=PAC:26...</td>\n",
       "      <td>None</td>\n",
       "      <td>PAC:26903339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chromosome_1</td>\n",
       "      <td>phytozome8_0</td>\n",
       "      <td>CDS</td>\n",
       "      <td>24126</td>\n",
       "      <td>28105</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>0</td>\n",
       "      <td>ID=PAC:26903339.CDS.1;Parent=PAC:26903339;paci...</td>\n",
       "      <td>None</td>\n",
       "      <td>PAC:26903339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chromosome_1</td>\n",
       "      <td>phytozome8_0</td>\n",
       "      <td>CDS</td>\n",
       "      <td>28291</td>\n",
       "      <td>28644</td>\n",
       "      <td>.</td>\n",
       "      <td>+</td>\n",
       "      <td>1</td>\n",
       "      <td>ID=PAC:26903339.CDS.2;Parent=PAC:26903339;paci...</td>\n",
       "      <td>None</td>\n",
       "      <td>PAC:26903339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     chromosome        source         feature  start    end score strand  \\\n",
       "0  chromosome_1  phytozome8_0            gene  24026  30617     .      +   \n",
       "1  chromosome_1  phytozome8_0            mRNA  24026  30617     .      +   \n",
       "2  chromosome_1  phytozome8_0  five_prime_UTR  24026  24125     .      +   \n",
       "3  chromosome_1  phytozome8_0             CDS  24126  28105     .      +   \n",
       "4  chromosome_1  phytozome8_0             CDS  28291  28644     .      +   \n",
       "\n",
       "  phase                                         attributes  \\\n",
       "0     .                ID=Cre01.g000050;Name=Cre01.g000050   \n",
       "1     .  ID=PAC:26903339;Name=Cre01.g000050.t1.3;pacid=...   \n",
       "2     .  ID=PAC:26903339.five_prime_UTR.1;Parent=PAC:26...   \n",
       "3     0  ID=PAC:26903339.CDS.1;Parent=PAC:26903339;paci...   \n",
       "4     1  ID=PAC:26903339.CDS.2;Parent=PAC:26903339;paci...   \n",
       "\n",
       "                 Name            ID  \n",
       "0                None          None  \n",
       "1  Cre01.g000050.t1.3  PAC:26903339  \n",
       "2                None  PAC:26903339  \n",
       "3                None  PAC:26903339  \n",
       "4                None  PAC:26903339  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get chromosome, start, and end positions of each five_prime_UTR, CDS, and three_prime_UTR in each PAC id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 197.6272006034851 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "exon_positions = []\n",
    "features_condition = ['five_prime_UTR', 'CDS', 'three_prime_UTR']\n",
    "#keep gff rows that contain CDS positions, remove gff rows that are missing PAC_ID's\n",
    "filtered_df = df[df.feature.isin(features_condition)].dropna(subset = ['ID'])\n",
    "#loop through genes in gff with PAC_ID's; transcript filtered by condition variable\n",
    "count = 0\n",
    "for pac_index in range(0,len(pac_id)): \n",
    "    \n",
    "    temp = filtered_df[filtered_df.ID==pac_id[pac_index]]\n",
    "    #print(transcript)\n",
    "    if len(temp)== 0: \n",
    "        exon_positions.append(None)\n",
    "        count+=0\n",
    "        \n",
    "    else:\n",
    "        exon_positions.append([])\n",
    "        #add CDS coordinates as tuple (chromosome, start, end) to gene_set\n",
    "        for i in list(temp.index):\n",
    "            exon_positions[pac_index].append((temp.chromosome[i], temp.start[i], temp.end[i]))\n",
    "        \n",
    "t1 = time.time()\n",
    "print(\"Time taken\", t1-t0, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PACid with missing  exon positions: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of PACid with missing  exon positions: \" + str(count) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export `exon_positions` to working folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/intermediate_data_02/exon_positions.pk', 'wb') as f:\n",
    "    pickle.dump(exon_positions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Add PAC ID to `sampled_genes.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = df[df.feature ==\"mRNA\"].sort_values(by=\"Name\").reset_index()\n",
    "search_list = list(subset.Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 12.20185136795044 s\n"
     ]
    }
   ],
   "source": [
    "#Order list of matching PAC id's to 'transcript_id_v5.3.1'\n",
    "#import re\n",
    "t0 = time.time()\n",
    "pacid_list = []\n",
    "\n",
    "for row in range(0,len(merged)):\n",
    "    transcript_id = merged.loc[row,'transcript_id_v5.3.1']\n",
    "    pacid = None\n",
    "    \n",
    "    index = sag.BinarySearch(search_list, transcript_id)\n",
    "    \n",
    "    if index > -1: \n",
    "        pacid = subset.loc[index, \"ID\"]\n",
    "                    \n",
    "    pacid_list.append(pacid)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time taken\", t1-t0, \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['PAC_id'] = pacid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_detected</th>\n",
       "      <th>num_manipulated</th>\n",
       "      <th>num_sampled</th>\n",
       "      <th>source</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>annotation_version</th>\n",
       "      <th>gene_id</th>\n",
       "      <th>gene_symbol</th>\n",
       "      <th>pathway_id</th>\n",
       "      <th>transcript_id_v5.3.1</th>\n",
       "      <th>PAC_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bajhaiya_2016</td>\n",
       "      <td>Cre01.g000017.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2.t1</td>\n",
       "      <td>PAC:26903746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Kwak_2017</td>\n",
       "      <td>Cre01.g000017.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g2.t1</td>\n",
       "      <td>PAC:26903746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bajhaiya_2016</td>\n",
       "      <td>Cre01.g000033.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>g3.t1</td>\n",
       "      <td>PAC:26903463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Gargouri_2015</td>\n",
       "      <td>Cre01.g000050.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000050.t1.3</td>\n",
       "      <td>PAC:26903339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bajhaiya_2016</td>\n",
       "      <td>Cre01.g000100.t1.1</td>\n",
       "      <td>v5.5</td>\n",
       "      <td>Cre01.g000100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cre01.g000100.t1.3</td>\n",
       "      <td>PAC:26903974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_detected  num_manipulated  num_sampled         source  \\\n",
       "0           0.0                0          1.0  Bajhaiya_2016   \n",
       "1           0.0                0          1.0      Kwak_2017   \n",
       "2           1.0                0          1.0  Bajhaiya_2016   \n",
       "3           0.0                0          1.0  Gargouri_2015   \n",
       "4           0.0                0          1.0  Bajhaiya_2016   \n",
       "\n",
       "        transcript_id annotation_version        gene_id gene_symbol  \\\n",
       "0  Cre01.g000017.t1.1               v5.5  Cre01.g000017         NaN   \n",
       "1  Cre01.g000017.t1.1               v5.5  Cre01.g000017         NaN   \n",
       "2  Cre01.g000033.t1.1               v5.5  Cre01.g000033         NaN   \n",
       "3  Cre01.g000050.t1.1               v5.5  Cre01.g000050         NaN   \n",
       "4  Cre01.g000100.t1.1               v5.5  Cre01.g000100         NaN   \n",
       "\n",
       "  pathway_id transcript_id_v5.3.1        PAC_id  \n",
       "0        NaN                g2.t1  PAC:26903746  \n",
       "1        NaN                g2.t1  PAC:26903746  \n",
       "2        NaN                g3.t1  PAC:26903463  \n",
       "3        NaN   Cre01.g000050.t1.3  PAC:26903339  \n",
       "4        NaN   Cre01.g000100.t1.3  PAC:26903974  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/workflow/sampled_genes.pk', 'wb') as f:\n",
    "    pickle.dump(merged, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define `SFSs_from_annotation()` to get SFS vectors for each transcript for neutral and selected sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exon_positions = pickle.load(open(\"../../data/intermediate_data_02/exon_positions.pk\", \"rb\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#This code is from Rob\n",
    "from ness_vcf import SFS\n",
    "from pysam import TabixFile\n",
    "from annotation import annotation_table\n",
    "\n",
    "\n",
    "def SFSs_from_annotation(coordinates, min_alleles=None, neutral_only=False): \n",
    "    annotation_tabix = TabixFile(filename=\"/scratch/research/references/chlamydomonas/5.3_chlamy_w_organelles_mt_minus/annotation/concatenated_GFF/annotation_table.txt.gz\")\n",
    "    \"\"\"\n",
    "    This function will return a dictionary of SFS objects\n",
    "    The dictionary will contain one SFS for each number of alleles that can be called\n",
    "        ie min_alleles to total number of individuals sequenced\n",
    "    It is possible to combine these SFSs by:\n",
    "        rounding MAF * (number of individuals sequenced) and keeping only one SFS\n",
    "    Arguments:\n",
    "     - take a TabixFile of the annotation table\n",
    "     - the chromosome, start and end (1-based inclusive)\n",
    "     - an optional minimum number of alleles - below this the site is shiite so don't take a bite\n",
    "     - neutral_only skips sites that aren't intergenic, intronic or 4-fold degenerate\n",
    "    \"\"\"\n",
    "    SFSs = {}\n",
    "    \n",
    "    #Loop through input list of (chromosome, start, end) from gff3\n",
    "    for i in coordinates:\n",
    "        chromosome, start, end = i\n",
    "        for line in annotation_tabix.fetch(chromosome, start-1, end):\n",
    "        # `annotation_line` is a class that has all the annotation table columns as attributes \n",
    "            a = annotation_table.annotation_line(line) #a has a lot of attributes\n",
    "            allele_counts = a.quebec_alleles\n",
    "            if neutral_only and sum([int(i) for i in [a.intergenic, a.intronic, a.fold4]]) == 0: #these are all neutral/silent sites\n",
    "                #because we are only looking at CDS, we will never get intergenic and intronic sequences\n",
    "                #our SFS in neutral  is a fold4 site\n",
    "                #fold0 sites go in selected SFS vector in est_dfe\n",
    "                continue\n",
    "            try:\n",
    "                MAF, total_alleles_called  = MAF_from_allele_count(allele_counts,min_alleles=min_alleles)\n",
    "                #if MAF > 0: print(MAF, total_alleles_called)\n",
    "            except TypeError: \n",
    "                continue\n",
    "            if min_alleles != None and total_alleles_called < min_alleles: #filter sites with too few alleles called\n",
    "                continue\n",
    "            if total_alleles_called not in SFSs: \n",
    "                #make SFS dictionary where key = n for SFS vector of length n\n",
    "                #you can't feed program with different alleles, so try to standardize and round them somehow in proportion to max alleles\n",
    "                SFSs[total_alleles_called] = SFS([0]*(total_alleles_called+1))\n",
    "            SFSs[total_alleles_called].add(MAF,total_alleles_called)\n",
    "\n",
    "\n",
    "    return SFSs\n",
    "\n",
    "\n",
    "def MAF_from_allele_count(allele_counts, min_alleles=None): # num of rare alleles/num of total alleles\n",
    "    \"\"\"\n",
    "    return the minor allele frequency and the number of called alleles\n",
    "    take a single allele_counts from annotation table ie, A:C:G:T    \n",
    "    optionally min_alleles will filter sites with too few alleles called\n",
    "    \"\"\"\n",
    "    minor_allele_count = sorted([int(i) for i in allele_counts.split(\":\")])[-2]\n",
    "    total_alleles_called = sum([int(i) for i in allele_counts.split(\":\")])\n",
    "    if min_alleles != None and total_alleles_called <= min_alleles:\n",
    "        return None\n",
    "    try:\n",
    "        MAF = minor_allele_count/float(total_alleles_called)\n",
    "        return (MAF,total_alleles_called)\n",
    "    except ZeroDivisionError:\n",
    "        return None\n",
    "    \n",
    "\n",
    "##annotation_table has clones filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code is from Rob\n",
    "from ness_vcf import SFS\n",
    "from pysam import TabixFile\n",
    "from annotation import annotation_table\n",
    "#our SFS in neutral  is a fold4 site\n",
    "#fold0 sites go in selected SFS vector in est_dfe\n",
    "\n",
    "def SFSs_from_annotation(coordinates, min_alleles=None, fold0_only=False, fold4_only=False): \n",
    "    annotation_tabix = TabixFile(filename=\"/scratch/research/references/chlamydomonas/5.3_chlamy_w_organelles_mt_minus/annotation/concatenated_GFF/annotation_table.txt.gz\")\n",
    "    \"\"\"\n",
    "    This function will return a dictionary of SFS objects\n",
    "    The dictionary will contain one SFS for each number of alleles that can be called\n",
    "        ie min_alleles to total number of individuals sequenced\n",
    "    It is possible to combine these SFSs by:\n",
    "        rounding MAF * (number of individuals sequenced) and keeping only one SFS\n",
    "    Arguments:\n",
    "     - take a TabixFile of the annotation table\n",
    "     - the chromosome, start and end (1-based inclusive)\n",
    "     - an optional minimum number of alleles - below this the site is shiite so don't take a bite\n",
    "     - neutral_only skips sites that aren't intergenic, intronic or 4-fold degenerate\n",
    "    \"\"\"\n",
    "    SFSs = {}\n",
    "    \n",
    "    #Loop through input list of (chromosome, start, end) from gff3\n",
    "    for i in coordinates:\n",
    "        chromosome, start, end = i\n",
    "        for line in annotation_tabix.fetch(chromosome, start-1, end):\n",
    "        # `annotation_line` is a class that has all the annotation table columns as attributes\n",
    "            #print(line.split())\n",
    "            a = annotation_table.annotation_line(line) #a has a lot of attributes\n",
    "            allele_counts = a.quebec_alleles\n",
    "            if fold0_only and int(a.fold0) == 0: #skip sites that are not fold0\n",
    "                continue\n",
    "            if fold4_only and int(a.fold4) == 0: #skip sites that are not fold4\n",
    "                continue\n",
    "            try:\n",
    "                MAF, total_alleles_called  = MAF_from_allele_count(allele_counts,min_alleles=min_alleles)\n",
    "                #if MAF > 0: print(MAF, total_alleles_called)\n",
    "            except TypeError: \n",
    "                continue\n",
    "            if min_alleles != None and total_alleles_called < min_alleles: #filter sites with too few alleles called\n",
    "                continue\n",
    "            if total_alleles_called not in SFSs: \n",
    "                #make SFS dictionary where key = n for SFS vector of length n\n",
    "                #you can't feed program with different alleles, so try to standardize and round them somehow in proportion to max alleles\n",
    "                SFSs[total_alleles_called] = SFS([0]*(total_alleles_called+1))\n",
    "            SFSs[total_alleles_called].add(MAF,total_alleles_called)\n",
    "\n",
    "\n",
    "    return SFSs\n",
    "\n",
    "\n",
    "def MAF_from_allele_count(allele_counts, min_alleles=None): # num of rare alleles/num of total alleles\n",
    "    \"\"\"\n",
    "    return the minor allele frequency and the number of called alleles\n",
    "    take a single allele_counts from annotation table ie, A:C:G:T    \n",
    "    optionally min_alleles will filter sites with too few alleles called\n",
    "    \"\"\"\n",
    "    minor_allele_count = sorted([int(i) for i in allele_counts.split(\":\")])[-2]\n",
    "    total_alleles_called = sum([int(i) for i in allele_counts.split(\":\")])\n",
    "    if min_alleles != None and total_alleles_called <= min_alleles:\n",
    "        return None\n",
    "    try:\n",
    "        MAF = minor_allele_count/float(total_alleles_called)\n",
    "        return (MAF,total_alleles_called)\n",
    "    except ZeroDivisionError:\n",
    "        return None\n",
    "    \n",
    "\n",
    "##annotation_table has clones filtered out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do SFS vectors from `SFS_annotation()` include invariant sites?\n",
    "SFS vectors from SFS dictionary output of `SFSs_from_annotation()` contains the number of invariants as the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = SFSs_from_annotation(exon_positions[1],min_alleles=12, fold0_only=True, fold4_only=False)\n",
    "neutral = SFSs_from_annotation(exon_positions[1],min_alleles=12, fold0_only=False, fold4_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alleles \t SFS vector\n",
      "16 \t\t [19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "17 \t\t [5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "18 \t\t [375, 8, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print (\"Alleles\", \"\\t\", \"SFS vector\")\n",
    "for key in selected.keys():\n",
    "    print (selected[key].alleles, \"\\t\\t\", selected[key].sfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alleles \t Invariant sites \t SFS vector\n",
      "16 \t\t 4 \t\t\t [4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "17 \t\t 2 \t\t\t [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "18 \t\t 120 \t\t\t [120, 8, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print (\"Alleles\", \"\\t\",\"Invariant sites\", \"\\t\", \"SFS vector\")\n",
    "for key in neutral.keys():\n",
    "    print (neutral[key].alleles, \"\\t\\t\", neutral[key].invariant(), \"\\t\\t\\t\", neutral[key].sfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define `reduce_SFS()` to reduce SFS vectors into one SFS vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_SFS(SFS_dict, max_alleles = 18):\n",
    "    \n",
    "    '''This function aggregates SFS vectors from SFSs_from_annotation() dictionary output into one SFS vector.\n",
    "    Default max_alleles = 18'''\n",
    "    \n",
    "    ref_SFS = range(0,max_alleles)\n",
    "    new_sfs = [0]*max_alleles\n",
    "    for key in SFS_dict.keys():\n",
    "        \n",
    "        sfs_length = len(SFS_dict[key].sfs)\n",
    "        #print(key)\n",
    "        normalized_index = [round(i/sfs_length*max_alleles)-1 for i in range(1,sfs_length+1)]\n",
    "        #print(normalized_index)\n",
    "        for i in range(0,sfs_length): new_sfs[normalized_index[i]]+=SFS_dict[key].sfs[i]\n",
    "        \n",
    "    return(new_sfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test `reduce_SFS()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce_SFS() logic worked as expected\n"
     ]
    }
   ],
   "source": [
    "#Test function logic\n",
    "max_alleles =10\n",
    "SFS_dict = {10: [1,0,1,0,1,0,1,0,1,0],\n",
    "           5: [1,0,1,0,1]}\n",
    "expected_output = [1,1,1,0,1,1,1,0,1,1]\n",
    "ref_SFS = range(0,max_alleles)\n",
    "new_sfs = [0]*max_alleles\n",
    "\n",
    "for key in SFS_dict.keys():\n",
    "        \n",
    "        sfs_length = len(SFS_dict[key])\n",
    "        #print(key)\n",
    "        normalized_index = [round(i/sfs_length*max_alleles)-1 for i in range(1,sfs_length+1)]\n",
    "        #print(normalized_index)\n",
    "        for i in range(0,sfs_length): new_sfs[normalized_index[i]]+=SFS_dict[key][i]\n",
    "\n",
    "if new_sfs==expected_output: print(\"reduce_SFS() logic worked as expected\")\n",
    "else: print(\"reduce_SFS() logic did not work as expected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126, 9, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test if collapse_SFS() returns error\n",
    "reduce_SFS(neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate time required to run `SFSs_from_annotation()` and `reduce_SFS()` on all transcripts. \n",
    "\n",
    "#### 1. One process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 genes took: 16.483078718185425 seconds.\n",
      "Estimated calculation time: 53.64967404790719 minutes\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "selected_SFS = []\n",
    "neutral_SFS = []\n",
    "#for i in range(len(exon_positions)):\n",
    "for i in range(0,100):\n",
    "    coordinates = exon_positions[i]\n",
    "    if coordinates != None:\n",
    "        selected = SFSs_from_annotation(coordinates, min_alleles=12, fold0_only=True, fold4_only=False)\n",
    "        selected_SFS.append(reduce_SFS(selected, max_alleles = 18))\n",
    "        neutral = SFSs_from_annotation(coordinates, min_alleles=12, fold0_only=False, fold4_only=True)\n",
    "        neutral_SFS.append(reduce_SFS(neutral, max_alleles = 18))\n",
    "t1 = time.time()\n",
    "print(\"100 genes took:\", t1-t0, \"seconds.\")\n",
    "print(\"Estimated calculation time:\", len(exon_positions)/100*(t1-t0)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [399, 9, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [921, 22, 15, 9, 3, 6, 1, 1, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2623, 26, 7, 5, 0, 4, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [478, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [892, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2015, 38, 10, 10, 3, 6, 2, 6, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [2000, 38, 10, 10, 3, 6, 2, 6, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [851, 4, 7, 4, 2, 1, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [126, 9, 0, 1, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0], [273, 12, 11, 3, 5, 6, 6, 3, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0], [791, 38, 16, 8, 8, 8, 8, 4, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0], [127, 4, 4, 1, 1, 6, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [228, 5, 3, 2, 3, 2, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [530, 31, 14, 16, 6, 13, 9, 5, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0], [526, 31, 14, 16, 6, 13, 9, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [263, 3, 7, 6, 2, 11, 4, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(selected_SFS[:10])\n",
    "print(neutral_SFS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. One process using multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 genes took: 16.567466020584106 seconds.\n",
      "Estimated calculation time: 53.9243406526645 minutes\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "def main():\n",
    "    pool = Pool(processes=1)\n",
    "    \n",
    "    exon_portion = exon_positions[0:100]\n",
    "    \n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, fold0_only=True, fold4_only=False)\n",
    "    selected = list(pool.map(func, exon_portion))\n",
    "    \n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, fold0_only=False, fold4_only=True)\n",
    "    neutral = list(pool.map(func, exon_portion))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    reduced_selected = list(pool.map(func, selected))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    reduced_neutral = list(pool.map(func, neutral))\n",
    "    \n",
    "    return reduced_selected, reduced_neutral\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    reduced_selected, reduced_neutral = main()\n",
    "    \n",
    "t1 = time.time()\n",
    "print(\"100 genes took:\", t1-t0, \"seconds.\")\n",
    "print(\"Estimated calculation time:\", len(exon_positions)/100*(t1-t0)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(reduced_selected[:10])\n",
    "print(reduced_neutral[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Eight processes using multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of CPUs: \", psutil.cpu_count(logical=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 genes took: 2.58024001121521 seconds.\n",
      "Estimated calculation time: 8.398251196503638 minutes\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "def main():\n",
    "    pool = Pool(processes=8)\n",
    "    \n",
    "    exon_portion = exon_positions[0:100]\n",
    "    \n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, fold0_only=True, fold4_only=False)\n",
    "    selected = list(pool.map(func, exon_portion))\n",
    "    \n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, fold0_only=False, fold4_only=True)\n",
    "    neutral = list(pool.map(func, exon_portion))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    reduced_selected = list(pool.map(func, selected))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    reduced_neutral = list(pool.map(func, neutral))\n",
    "    \n",
    "    return reduced_selected, reduced_neutral\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    reduced_selected, reduced_neutral = main()\n",
    "    \n",
    "t1 = time.time()\n",
    "print(\"100 genes took:\", t1-t0, \"seconds.\")\n",
    "print(\"Estimated calculation time:\", len(exon_positions)/100*(t1-t0)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(reduced_selected[:10])\n",
    "print(reduced_neutral[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the multiprocessing script on all transcripts in `exon_positions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script took: 490.44499588012695 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "def main():\n",
    "    pool = Pool(processes=8)\n",
    "    \n",
    "    exon_portion = exon_positions\n",
    "    \n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, fold0_only=True, fold4_only=False)\n",
    "    selected = list(pool.map(func, exon_portion))\n",
    "    \n",
    "    func = partial(SFSs_from_annotation, min_alleles=12, fold0_only=False, fold4_only=True)\n",
    "    neutral = list(pool.map(func, exon_portion))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    reduced_selected = list(pool.map(func, selected))\n",
    "    \n",
    "    func = partial(reduce_SFS, max_alleles = 18)\n",
    "    reduced_neutral = list(pool.map(func, neutral))\n",
    "    \n",
    "    return reduced_selected, reduced_neutral\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    reduced_selected, reduced_neutral = main()\n",
    "    \n",
    "t1 = time.time()\n",
    "print(\"Script took:\", t1-t0, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create dataframe of reduced SFS for each transcript.\n",
    "\n",
    "Export `reduced_SFS` dataframe as `../../data/intermediate_data_02/reduced_SFS.pk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_SFS = pd.DataFrame({'PAC_id': pac_id,\n",
    "              'selected_SFS': reduced_selected,\n",
    "              'neutral_SFS': reduced_neutral})\n",
    "\n",
    "with open('../../data/intermediate_data_02/reduced_SFS.pk', 'wb') as f:\n",
    "    pickle.dump(reduced_SFS, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
